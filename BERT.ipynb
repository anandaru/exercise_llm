{"cells":[{"cell_type":"markdown","metadata":{"id":"lm3IUah0bL8h"},"source":["#BERT (Bidirectional Encoder Representations from Transformers) Model"]},{"cell_type":"markdown","metadata":{"id":"h0bZvl7Nsyr4"},"source":["## Load train data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33557,"status":"ok","timestamp":1727172719532,"user":{"displayName":"Arumilli Anand","userId":"07253633933178490233"},"user_tz":-330},"id":"q3Xwke2-CP7q","outputId":"051e5533-ff0d-4956-90d4-d62f24d1a78b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# prompt: load from google drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1943,"status":"ok","timestamp":1727172725223,"user":{"displayName":"Arumilli Anand","userId":"07253633933178490233"},"user_tz":-330},"id":"ogDWkIx-CJLG","outputId":"d8fb2dc5-1f86-4fc5-be5a-ccac329406ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'sentence': 'i did ab work it was gasy', 'exercise': 'ab crunches', 'feeling': 'gasy'}\n","{'sentence': 'i did two hundred ab crunches over five sets. the last 50 reps were terrible, i felt so tired.', 'exercise': 'ab crunches', 'feeling': 'terrible, tired'}\n","{'sentence': '20 ab crunches for senior citizen struggling after the first 5', 'exercise': 'ab crunches', 'feeling': 'struggling'}\n","{'sentence': 'i did ab crunches. i felt comfortable and happy to do this movement.', 'exercise': 'ab crunches', 'feeling': 'comfortable, happy'}\n","{'sentence': 'i used the ab machine and did 10 reps. i feel strong.', 'exercise': 'ab machine', 'feeling': 'strong'}\n"]}],"source":["import pandas as pd\n","\n","# Load the CSV file\n","path = '/content/drive/MyDrive//pcems/data.csv'\n","df_train = pd.read_csv(path)\n","\n","# Convert the DataFrame to a list of dictionaries\n","train_data = df_train.to_dict(orient='records')\n","\n","# Display the first few samples to verify\n","for sample in train_data[:5]:\n","    print(sample)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m5R5fgkJdAUh"},"outputs":[],"source":["df_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PakWk1zE3VTI"},"outputs":[],"source":["df_train.shape"]},{"cell_type":"markdown","metadata":{"id":"9ENT6TNBsnZx"},"source":["##Define Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"17dS0mXNUYzg"},"source":["##BERT (Bidirectional Encoder Representations from Transformers) Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pAVNCj-aUUuQ"},"outputs":[],"source":["from transformers import BertForSequenceClassification, BertTokenizer\n","\n","model_name = \"bert-base-uncased\"\n","model = BertForSequenceClassification.from_pretrained(model_name)\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cm3VBoawejCv"},"outputs":[],"source":["import pandas as pd\n","import torch\n","from transformers import BertForSequenceClassification, BertTokenizer\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score\n","\n","# Prepare the DataFrame (Assume df_train is already loaded)\n","formatted_data = [\n","    {\n","        \"input_text\": f\"classify: {row['sentence']}\",\n","        \"target_text\": f\"feeling: {row['feeling']}, exercise: {row['exercise']}\"\n","    }\n","    for idx, row in df_train.iterrows()\n","]\n","\n","# Extract input and target texts\n","input_texts = [sample[\"input_text\"] for sample in formatted_data]\n","target_texts = [sample[\"target_text\"] for sample in formatted_data]\n","\n","# Encode target texts to labels\n","label_encoder = LabelEncoder()\n","labels = label_encoder.fit_transform(target_texts)\n","\n","# Custom Dataset class for BERT\n","class CustomDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, index):\n","        text = self.texts[index]\n","        label = self.labels[index]\n","\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","\n","        return {\n","            'text': text,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'label': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","# Initialize the tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_encoder.classes_))\n","\n","# Create DataLoader\n","dataset = CustomDataset(input_texts, labels, tokenizer, max_len=128)\n","dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n","\n","# Fine-tune the model\n","model.train()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n","\n","for epoch in range(70):  # Adjust the number of epochs for fine-tuning\n","    total_loss = 0\n","    for batch in dataloader:\n","        optimizer.zero_grad()\n","        outputs = model(\n","            input_ids=batch['input_ids'],\n","            attention_mask=batch['attention_mask'],\n","            labels=batch['label']\n","        )\n","\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(dataloader)\n","    print(f\"Epoch {epoch + 1}, Loss: {avg_loss}\")\n","\n","# Function to classify new text using the fine-tuned model\n","def classify_text(text):\n","    model.eval()\n","    input_text = f\"classify: {text}\"\n","    encoding = tokenizer.encode_plus(\n","        input_text,\n","        add_special_tokens=True,\n","        max_length=128,\n","        return_token_type_ids=False,\n","        padding='max_length',\n","        truncation=True,\n","        return_attention_mask=True,\n","        return_tensors='pt',\n","    )\n","    input_ids = encoding['input_ids']\n","    attention_mask = encoding['attention_mask']\n","\n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        predicted_class_id = torch.argmax(logits, dim=-1).item()\n","\n","    return label_encoder.inverse_transform([predicted_class_id])[0]\n","\n","# Example usage with the first five logs from the DataFrame\n","for idx, row in df_train.head(5).iterrows():\n","    new_log = row['sentence']\n","    classification = classify_text(new_log)\n","    print(f\"Log: {new_log}\\nClassification: {classification}\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"ctWMYP5DsqFZ"},"source":["##Train Data"]},{"cell_type":"markdown","metadata":{"id":"swfaYnU94cdC"},"source":["##Load Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1844,"status":"ok","timestamp":1727179214760,"user":{"displayName":"Arumilli Anand","userId":"07253633933178490233"},"user_tz":-330},"id":"I45Hk04S4eyv","outputId":"999a984c-7203-4c29-a36f-c4a944daa10b"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'sentence': 'i did ab work it was gasy', 'exercise': 'ab crunches', 'feeling': 'gasy'}\n","{'sentence': 'i did two hundred ab crunches over five sets. the last 50 reps were terrible, i felt so tired.', 'exercise': 'ab crunches', 'feeling': 'terrible, tired'}\n","{'sentence': 'i did 30 minutes of aerobics i felt exhausted', 'exercise': 'aerobics', 'feeling': 'exhausted'}\n","{'sentence': 'i just finished a one hour water aerobics class. i felt light, like all the pressure was taken off my body during the session.', 'exercise': 'aerobics', 'feeling': 'light'}\n","{'sentence': 'i played badminton for more than two hours, i felt tired and legs and foot are paining like hell.', 'exercise': 'badminton', 'feeling': 'tired, legs and foot are paining'}\n"]}],"source":["# Load the CSV file\n","path = '/content/drive/MyDrive//pcems/data_test.csv'\n","\n","df_test = pd.read_csv(path, encoding='latin-1')\n","\n","# Convert the DataFrame to a list of dictionaries\n","test_data = df_test.to_dict(orient='records')\n","\n","# Display the first few samples to verify\n","for sample in test_data[:5]:\n","    print(sample)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t2CkImfknTau"},"outputs":[],"source":["for idx, row in df_test.head(5).iterrows():\n","    new_log = row['sentence']\n","    classification = classify_text(new_log)\n","    print(f\"sentence: {new_log}\\nClassification: {classification}\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VQMpb3b4sRA9"},"outputs":[],"source":["df_test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YHQnqHI--i4I"},"outputs":[],"source":["for sample in test_data[:20]:\n","  new_log = sample['sentence']\n","  classification = classify_text(new_log)\n","  print(f\"sentence: {new_log} --- Classification: {classification}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LyKQFtE2-GK7"},"outputs":[],"source":["# Create new columns for predicted exercise and feeling\n","df_test['predicted_Exercise'] = ''\n","df_test['predicted_Feeling'] = ''\n","\n","# prompt: handle the above error. if error then         df_test.at[idx, 'predicted_Feeling'] = 'none'\n","#         df_test.at[idx, 'predicted_Exercise'] = 'none' and continue\n","\n","# Iterate through each row and classify the log text\n","for idx, row in df_test.iterrows():\n","    new_log = row['sentence']\n","    try:\n","        classification = classify_text(new_log)\n","\n","        # Check if classify_text returns a string and extract relevant information\n","        if isinstance(classification, str) and ',' in classification:\n","            # Assuming the format is 'feeling: <feeling>, exercise: <exercise>'\n","            parts = classification.split(',')\n","            for part in parts:\n","                key, value = part.strip().split(': ')\n","                if key == 'feeling':\n","                    df_test.at[idx, 'predicted_Feeling'] = value\n","                elif key == 'exercise':\n","                    df_test.at[idx, 'predicted_Exercise'] = value\n","        else:\n","            # Set 'none' for both columns if classification format is not as expected\n","            df_test.at[idx, 'predicted_Feeling'] = 'none'\n","            df_test.at[idx, 'predicted_Exercise'] = 'none'\n","    except:\n","        # If an error occurs during classification, set both predicted columns to 'none'\n","        df_test.at[idx, 'predicted_Feeling'] = 'none'\n","        df_test.at[idx, 'predicted_Exercise'] = 'none'\n","        continue\n","\n","    # Print for debugging purposes (optional)\n","    print(f\"sentence: {new_log}\\nClassification: {classification}\\n\")\n"]},{"cell_type":"code","source":["# Create new columns for predicted exercise and feeling\n","df_test['predicted_Exercise'] = ''\n","df_test['predicted_Feeling'] = ''\n","\n","# Iterate through each row and classify the log text\n","for idx, row in df_test.iterrows():\n","    new_log = row['sentence']\n","    try:\n","        classification = classify_text(new_log)\n","\n","        # Ensure classification is a string and contains a comma (expected format)\n","        if isinstance(classification, str) and ',' in classification:\n","            # Strip any extra spaces or periods and split the parts\n","            parts = [part.strip().rstrip('.') for part in classification.split(',')]\n","\n","            # Loop through the parts to find the correct key-value pairs\n","            for part in parts:\n","                if ': ' in part:\n","                    key, value = part.split(': ', 1)\n","                    key, value = key.strip(), value.strip()  # Ensure no extra spaces\n","                    if key == 'feeling':\n","                        df_test.at[idx, 'predicted_Feeling'] = value\n","                    elif key == 'exercise':\n","                        df_test.at[idx, 'predicted_Exercise'] = value\n","        else:\n","            # If the classification format is not as expected, set 'none'\n","            df_test.at[idx, 'predicted_Feeling'] = 'none'\n","            df_test.at[idx, 'predicted_Exercise'] = 'none'\n","\n","    except Exception as e:\n","        # If an error occurs, set both predicted columns to 'none' and continue\n","        df_test.at[idx, 'predicted_Feeling'] = 'none'\n","        df_test.at[idx, 'predicted_Exercise'] = 'none'\n","        print(f\"Error: {e} for sentence: {new_log}\")  # Optional: log the error\n","        continue\n","\n","    # Print for debugging purposes (optional)\n","    print(f\"sentence: {new_log}\\nClassification: {classification}\\n\")\n"],"metadata":{"id":"UZzu0DWe4MyY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7gKWd-jd-u7S"},"outputs":[],"source":["df_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xqYen-8mQv8E"},"outputs":[],"source":["# prompt: accuracy = sum(correct_feeling)/total no of records\n","\n","accuracy_feeling = df_test['correct_feeling'].sum() / len(df_test)\n","print(f\"Accuracy for feeling classification: {accuracy_feeling:.2f}\")\n","\n","accuracy_exercise = df_test['correct_exercise'].sum() / len(df_test)\n","print(f\"Accuracy for exercise classification: {accuracy_exercise:.2f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"LyDPCFylWqrz"},"source":["##Record Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jGl8EyMdWs3c"},"outputs":[],"source":["# Initialize results dictionary\n","results = {\"Model\": [], \"feeling_Accuracy\": [], \"exercise_Accuracy\": []}\n","\n","results[\"Model\"].append('BERT')\n","results[\"feeling_Accuracy\"].append(0.69)\n","results[\"exercise_Accuracy\"].append(0.61)\n","\n","# Convert results to DataFrame\n","df_results = pd.DataFrame(results)\n","df_results"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[{"file_id":"1ZUTMvyy_2eEJ_r1BV064oj2Bi3MRb8qm","timestamp":1727628428769}],"authorship_tag":"ABX9TyPGiYxZK8XOn9tmV0SCvJgD"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}