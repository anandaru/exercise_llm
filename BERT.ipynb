{"cells":[{"cell_type":"markdown","metadata":{"id":"lm3IUah0bL8h"},"source":["#BERT (Bidirectional Encoder Representations from Transformers) Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E4pNKipAbHJp"},"outputs":[],"source":["pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PxtNahXPbQck"},"outputs":[],"source":["pip install langchain_community"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UDJwHlMpbUSx"},"outputs":[],"source":["!pip install sentence_transformers"]},{"cell_type":"markdown","metadata":{"id":"h0bZvl7Nsyr4"},"source":["## Load train data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q3Xwke2-CP7q"},"outputs":[],"source":["# prompt: load from google drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ogDWkIx-CJLG"},"outputs":[],"source":["import pandas as pd\n","\n","# Load the CSV file\n","#path = '/content/drive/MyDrive//pcems/trainMajorityVoteData1.csv'\n","path = '/content/drive/MyDrive//pcems/data.csv'\n","df_train = pd.read_csv(path)\n","\n","# Convert the DataFrame to a list of dictionaries\n","train_data = df_train.to_dict(orient='records')\n","\n","# Display the first few samples to verify\n","for sample in train_data[:5]:\n","    print(sample)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m5R5fgkJdAUh"},"outputs":[],"source":["df_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PakWk1zE3VTI"},"outputs":[],"source":["df_train.shape"]},{"cell_type":"markdown","metadata":{"id":"9ENT6TNBsnZx"},"source":["##Define Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"17dS0mXNUYzg"},"source":["##BERT (Bidirectional Encoder Representations from Transformers) Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pAVNCj-aUUuQ"},"outputs":[],"source":["from transformers import BertForSequenceClassification, BertTokenizer\n","\n","model_name = \"bert-base-uncased\"\n","model = BertForSequenceClassification.from_pretrained(model_name)\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cm3VBoawejCv"},"outputs":[],"source":["import pandas as pd\n","import torch\n","from transformers import BertForSequenceClassification, BertTokenizer\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score\n","import matplotlib.pyplot as plt\n","\n","# Prepare the DataFrame (Assume df_train is already loaded)\n","formatted_data = [\n","    {\n","        \"input_text\": f\"classify: {row['sentence']}\",\n","        \"target_text\": f\"feeling: {row['feeling']}, exercise: {row['exercise']}\"\n","    }\n","    for idx, row in df_train.iterrows()\n","]\n","\n","# Extract input and target texts\n","input_texts = [sample[\"input_text\"] for sample in formatted_data]\n","target_texts = [sample[\"target_text\"] for sample in formatted_data]\n","\n","# Encode target texts to labels\n","label_encoder = LabelEncoder()\n","labels = label_encoder.fit_transform(target_texts)\n","\n","# Custom Dataset class for BERT\n","class CustomDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, index):\n","        text = self.texts[index]\n","        label = self.labels[index]\n","\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","\n","        return {\n","            'text': text,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'label': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","# Initialize the tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_encoder.classes_))\n","\n","# Create DataLoader\n","dataset = CustomDataset(input_texts, labels, tokenizer, max_len=128)\n","dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n","\n","# Initialize a list to store the loss values for each epoch\n","loss_values = []\n","\n","# Fine-tune the model\n","model.train()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n","\n","for epoch in range(70):  # Adjust the number of epochs for fine-tuning\n","    total_loss = 0\n","    for batch in dataloader:\n","        optimizer.zero_grad()\n","        outputs = model(\n","            input_ids=batch['input_ids'],\n","            attention_mask=batch['attention_mask'],\n","            labels=batch['label']\n","        )\n","\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(dataloader)\n","    loss_values.append(avg_loss)  # Append the average loss for this epoch\n","    print(f\"Epoch {epoch + 1}, Loss: {avg_loss}\")\n","\n","# Plot the loss curve\n","plt.figure(figsize=(10, 6))\n","plt.plot(range(1, 71), loss_values, label='Training Loss', color='blue', marker='o')\n","plt.title('Loss Curve Over 70 Epochs')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.grid(True)\n","plt.legend()\n","plt.show()\n","# Function to classify new text using the fine-tuned model\n","\n","def classify_text(text):\n","    model.eval()\n","    input_text = f\"classify: {text}\"\n","    encoding = tokenizer.encode_plus(\n","        input_text,\n","        add_special_tokens=True,\n","        max_length=128,\n","        return_token_type_ids=False,\n","        padding='max_length',\n","        truncation=True,\n","        return_attention_mask=True,\n","        return_tensors='pt',\n","    )\n","    input_ids = encoding['input_ids']\n","    attention_mask = encoding['attention_mask']\n","\n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        predicted_class_id = torch.argmax(logits, dim=-1).item()\n","\n","    return label_encoder.inverse_transform([predicted_class_id])[0]\n","\n","# Example usage with the first five logs from the DataFrame\n","for idx, row in df_train.head(5).iterrows():\n","    new_log = row['sentence']\n","    classification = classify_text(new_log)\n","    print(f\"Log: {new_log}\\nClassification: {classification}\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"ctWMYP5DsqFZ"},"source":["##Train Data"]},{"cell_type":"markdown","metadata":{"id":"swfaYnU94cdC"},"source":["##Load Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I45Hk04S4eyv"},"outputs":[],"source":["# Load the CSV file\n","#path = '/content/drive/MyDrive//pcems/testMajorityVoteData1.csv'\n","#path = '/content/drive/MyDrive//pcems/strat_test_set.csv'\n","path = '/content/drive/MyDrive//pcems/data_27.csv'\n","\n","df_test = pd.read_csv(path, encoding='latin-1')\n","\n","# Convert the DataFrame to a list of dictionaries\n","test_data = df_test.to_dict(orient='records')\n","\n","# Display the first few samples to verify\n","for sample in test_data[:5]:\n","    print(sample)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t2CkImfknTau"},"outputs":[],"source":["for idx, row in df_test.head(5).iterrows():\n","    new_log = row['sentence']\n","    classification = classify_text(new_log)\n","    print(f\"sentence: {new_log}\\nClassification: {classification}\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VQMpb3b4sRA9"},"outputs":[],"source":["df_test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YHQnqHI--i4I"},"outputs":[],"source":["for sample in test_data[:20]:\n","  new_log = sample['sentence']\n","  classification = classify_text(new_log)\n","  print(f\"sentence: {new_log} --- Classification: {classification}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LyKQFtE2-GK7"},"outputs":[],"source":["# Create new columns for predicted exercise and feeling\n","df_test['predicted_Exercise'] = ''\n","df_test['predicted_Feeling'] = ''\n","\n","# prompt: handle the above error. if error then         df_test.at[idx, 'predicted_Feeling'] = 'none'\n","#         df_test.at[idx, 'predicted_Exercise'] = 'none' and continue\n","\n","# Iterate through each row and classify the log text\n","for idx, row in df_test.iterrows():\n","    new_log = row['sentence']\n","    try:\n","        classification = classify_text(new_log)\n","\n","        # Check if classify_text returns a string and extract relevant information\n","        if isinstance(classification, str) and ',' in classification:\n","            # Assuming the format is 'feeling: <feeling>, exercise: <exercise>'\n","            parts = classification.split(',')\n","            for part in parts:\n","                key, value = part.strip().split(': ')\n","                if key == 'feeling':\n","                    df_test.at[idx, 'predicted_Feeling'] = value\n","                elif key == 'exercise':\n","                    df_test.at[idx, 'predicted_Exercise'] = value\n","        else:\n","            # Set 'none' for both columns if classification format is not as expected\n","            df_test.at[idx, 'predicted_Feeling'] = 'none'\n","            df_test.at[idx, 'predicted_Exercise'] = 'none'\n","    except:\n","        # If an error occurs during classification, set both predicted columns to 'none'\n","        df_test.at[idx, 'predicted_Feeling'] = 'none'\n","        df_test.at[idx, 'predicted_Exercise'] = 'none'\n","        continue\n","\n","    # Print for debugging purposes (optional)\n","    print(f\"sentence: {new_log}\\nClassification: {classification}\\n\")\n"]},{"cell_type":"code","source":["# Create new columns for predicted exercise and feeling\n","df_test['predicted_Exercise'] = ''\n","df_test['predicted_Feeling'] = ''\n","\n","# Iterate through each row and classify the log text\n","for idx, row in df_test.iterrows():\n","    new_log = row['sentence']\n","    try:\n","        classification = classify_text(new_log)\n","\n","        # Ensure classification is a string and contains a comma (expected format)\n","        if isinstance(classification, str) and ',' in classification:\n","            # Strip any extra spaces or periods and split the parts\n","            parts = [part.strip().rstrip('.') for part in classification.split(',')]\n","\n","            # Loop through the parts to find the correct key-value pairs\n","            for part in parts:\n","                if ': ' in part:\n","                    key, value = part.split(': ', 1)\n","                    key, value = key.strip(), value.strip()  # Ensure no extra spaces\n","                    if key == 'feeling':\n","                        df_test.at[idx, 'predicted_Feeling'] = value\n","                    elif key == 'exercise':\n","                        df_test.at[idx, 'predicted_Exercise'] = value\n","        else:\n","            # If the classification format is not as expected, set 'none'\n","            df_test.at[idx, 'predicted_Feeling'] = 'none'\n","            df_test.at[idx, 'predicted_Exercise'] = 'none'\n","\n","    except Exception as e:\n","        # If an error occurs, set both predicted columns to 'none' and continue\n","        df_test.at[idx, 'predicted_Feeling'] = 'none'\n","        df_test.at[idx, 'predicted_Exercise'] = 'none'\n","        print(f\"Error: {e} for sentence: {new_log}\")  # Optional: log the error\n","        continue\n","\n","    # Print for debugging purposes (optional)\n","    print(f\"sentence: {new_log}\\nClassification: {classification}\\n\")\n"],"metadata":{"id":"UZzu0DWe4MyY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","# Assuming df_test is already defined and contains the test data with predicted labels\n","\n","# Load a pre-trained model for embeddings\n","model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n","\n","# Define a function to compute similarity\n","def compute_similarity(predicted, actual):\n","    predicted_embedding = model.encode(predicted)\n","    actual_embedding = model.encode(actual)\n","    similarity = cosine_similarity([predicted_embedding], [actual_embedding])\n","    return similarity[0][0]\n","\n","# Initialize columns\n","df_test['feeling_similarity'] = 0.0\n","df_test['exercise_similarity'] = 0.0\n","df_test['avg_similarity'] = 0.0\n","df_test['correct_feeling'] = 0\n","df_test['correct_exercise'] = 0\n","\n","# Define threshold\n","similarity_threshold = 0.8\n","\n","# Compute similarity for each pair of predictions and actual labels\n","for idx, row in df_test.iterrows():\n","    actual_feeling = row['feeling']\n","    actual_exercise = row['exercise']\n","    predicted_feeling = row['predicted_Feeling']\n","    predicted_exercise = row['predicted_Exercise']\n","\n","    feeling_similarity = compute_similarity(predicted_feeling, actual_feeling)\n","    exercise_similarity = compute_similarity(predicted_exercise, actual_exercise)\n","\n","    df_test.at[idx, 'feeling_similarity'] = feeling_similarity\n","    df_test.at[idx, 'exercise_similarity'] = exercise_similarity\n","    df_test.at[idx, 'avg_similarity'] = (feeling_similarity + exercise_similarity) / 2\n","\n","    df_test.at[idx, 'correct_feeling'] = 1 if feeling_similarity > similarity_threshold else 0\n","    df_test.at[idx, 'correct_exercise'] = 1 if exercise_similarity > similarity_threshold else 0"],"metadata":{"id":"zEdX89v4b0DE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7gKWd-jd-u7S"},"outputs":[],"source":["df_test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":620,"status":"ok","timestamp":1727179412114,"user":{"displayName":"Arumilli Anand","userId":"07253633933178490233"},"user_tz":-330},"id":"xqYen-8mQv8E","outputId":"9f86ccff-9659-4afa-bffa-deb60564011c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy for feeling classification: 0.69\n","Accuracy for exercise classification: 0.61\n"]}],"source":["# prompt: accuracy = sum(correct_feeling)/total no of records\n","\n","accuracy_feeling = df_test['correct_feeling'].sum() / len(df_test)\n","print(f\"Accuracy for feeling classification: {accuracy_feeling:.2f}\")\n","\n","accuracy_exercise = df_test['correct_exercise'].sum() / len(df_test)\n","print(f\"Accuracy for exercise classification: {accuracy_exercise:.2f}\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[{"file_id":"1ZUTMvyy_2eEJ_r1BV064oj2Bi3MRb8qm","timestamp":1727674863530}],"authorship_tag":"ABX9TyOEEaRSDoc2IoCNMITOs3Fz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}