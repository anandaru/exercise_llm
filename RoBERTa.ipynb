{"cells":[{"cell_type":"markdown","metadata":{"id":"lm3IUah0bL8h"},"source":["#RoBERTa (A Robustly Optimized BERT Pretraining Approach) Model"]},{"cell_type":"markdown","metadata":{"id":"h0bZvl7Nsyr4"},"source":["## Load train data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21053,"status":"ok","timestamp":1727615666511,"user":{"displayName":"Arumilli Anand","userId":"07253633933178490233"},"user_tz":-330},"id":"q3Xwke2-CP7q","outputId":"d565d611-8add-495d-99c8-f94074501219"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# prompt: load from google drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1730,"status":"ok","timestamp":1727615671409,"user":{"displayName":"Arumilli Anand","userId":"07253633933178490233"},"user_tz":-330},"id":"ogDWkIx-CJLG","outputId":"4316d80c-6f64-4a16-a7f8-8f86a7da2b4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'sentence': 'i did ab work it was gasy', 'exercise': 'ab crunches', 'feeling': 'gasy'}\n","{'sentence': 'i did two hundred ab crunches over five sets. the last 50 reps were terrible, i felt so tired.', 'exercise': 'ab crunches', 'feeling': 'terrible, tired'}\n","{'sentence': '20 ab crunches for senior citizen struggling after the first 5', 'exercise': 'ab crunches', 'feeling': 'struggling'}\n","{'sentence': 'i did ab crunches. i felt comfortable and happy to do this movement.', 'exercise': 'ab crunches', 'feeling': 'comfortable, happy'}\n","{'sentence': 'i used the ab machine and did 10 reps. i feel strong.', 'exercise': 'ab machine', 'feeling': 'strong'}\n"]}],"source":["import pandas as pd\n","\n","# Load the CSV file\n","path = '/content/drive/MyDrive//pcems/data.csv'\n","df_train = pd.read_csv(path)\n","\n","# Convert the DataFrame to a list of dictionaries\n","train_data = df_train.to_dict(orient='records')\n","\n","# Display the first few samples to verify\n","for sample in train_data[:5]:\n","    print(sample)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m5R5fgkJdAUh"},"outputs":[],"source":["df_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":472,"status":"ok","timestamp":1727615681295,"user":{"displayName":"Arumilli Anand","userId":"07253633933178490233"},"user_tz":-330},"id":"PakWk1zE3VTI","outputId":"fe203519-e3bd-4edf-c910-5ca0bba75b92"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(200, 3)"]},"metadata":{},"execution_count":7}],"source":["df_train.shape"]},{"cell_type":"markdown","metadata":{"id":"9ENT6TNBsnZx"},"source":["##Define Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"17dS0mXNUYzg"},"source":["##RoBERTa (A Robustly Optimized BERT Pretraining Approach) Model\n","\n","RoBERTa (A Robustly Optimized BERT Pretraining Approach) is an advanced language representation model developed by Facebook AI. It builds upon the BERT (Bidirectional Encoder Representations from Transformers) architecture with several key improvements aimed at enhancing performance and robustness. Here are the main features and enhancements of RoBERTa:\n","\n","Key Features of RoBERTa:\n","Training Data: RoBERTa uses a much larger dataset for pre-training compared to BERT. It is trained on a combination of datasets, including the BookCorpus, English Wikipedia, Common Crawl News, OpenWebText, and Stories from Common Crawl.\n","\n","Training Time and Batch Size: RoBERTa increases the amount of training time and the batch size. This allows the model to learn more effectively from the data.\n","\n","Dynamic Masking: Unlike BERT, which uses a static masking pattern for its masked language modeling task, RoBERTa applies dynamic masking, meaning the masking pattern changes during each epoch of training. This helps the model to learn better representations.\n","\n","No Next Sentence Prediction: RoBERTa removes the Next Sentence Prediction (NSP) objective used in BERT. Research indicated that NSP might not be necessary and could even be detrimental. Instead, RoBERTa focuses solely on the masked language modeling task.\n","\n","Larger Batch Sizes and Learning Rates: RoBERTa uses larger batch sizes and learning rates during training, which contributes to more robust and effective learning.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cBPBPeuDXOWW"},"source":["* Prepare the DataFrame: This involves structuring the data for few-shot learning by creating formatted input text with K examples for conditioning.\n","* Create the Custom Dataset: This will handle the tokenization and preparation of the data for the model.\n","* Fine-tune the Model: Train the model on the few-shot learning setup.\n","* Evaluate the Model: Evaluate the model on the test set using beam search for tasks requiring free-form completion."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tN3Ex1jJXN0L"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cm3VBoawejCv"},"outputs":[],"source":["import pandas as pd\n","import torch\n","from transformers import RobertaForSequenceClassification, RobertaTokenizer\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score\n","import random\n","\n","# Prepare the DataFrame (Assume df_train and df_dev are already loaded)\n","K = 90  # Number of conditioning examples\n","\n","def prepare_few_shot_data(df_train, K):\n","    few_shot_data = []\n","    for idx, row in df_train.iterrows():\n","        conditioning_examples = random.sample(df_train.to_dict('records'), K)\n","        conditioning_text = \"\\n\\n\".join([f\"classify: {ex['sentence']}\\nfeeling: {ex['feeling']}, exercise: {ex['exercise']}\" for ex in conditioning_examples])\n","        input_text = f\"{conditioning_text}\\n\\nclassify: {row['sentence']}\"\n","        target_text = f\"feeling: {row['feeling']}, exercise: {row['exercise']}\"\n","        few_shot_data.append({\"input_text\": input_text, \"target_text\": target_text})\n","    return few_shot_data\n","\n","few_shot_data = prepare_few_shot_data(df_train, K)\n","\n","# Extract input and target texts\n","input_texts = [sample[\"input_text\"] for sample in few_shot_data]\n","target_texts = [sample[\"target_text\"] for sample in few_shot_data]\n","\n","# Encode target texts to labels\n","label_encoder = LabelEncoder()\n","labels = label_encoder.fit_transform(target_texts)\n","\n","# Custom Dataset class for RoBERTa\n","class CustomDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, index):\n","        text = self.texts[index]\n","        label = self.labels[index]\n","\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","\n","        return {\n","            'text': text,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'label': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","# Initialize the tokenizer and model\n","tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_encoder.classes_))\n","\n","# Create DataLoader\n","dataset = CustomDataset(input_texts, labels, tokenizer, max_len=128)\n","dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n","\n","# Fine-tune the model\n","model.train()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n","\n","for epoch in range(60):  # Adjust the number of epochs for fine-tuning\n","    total_loss = 0\n","    for batch in dataloader:\n","        optimizer.zero_grad()\n","        outputs = model(\n","            input_ids=batch['input_ids'],\n","            attention_mask=batch['attention_mask'],\n","            labels=batch['label']\n","        )\n","\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(dataloader)\n","    print(f\"Epoch {epoch + 1}, Loss: {avg_loss}\")\n","\n","# Function to classify new text using the fine-tuned model\n","def classify_text(text, conditioning_examples):\n","    model.eval()\n","    conditioning_text = \"\\n\\n\".join([f\"classify: {ex['Log']}\\nfeeling: {ex['Feeling Tag']}, exercise: {ex['Exercise Tag']}\" for ex in conditioning_examples])\n","    input_text = f\"{conditioning_text}\\n\\nclassify: {text}\"\n","    encoding = tokenizer.encode_plus(\n","        input_text,\n","        add_special_tokens=True,\n","        max_length=128,\n","        return_token_type_ids=False,\n","        padding='max_length',\n","        truncation=True,\n","        return_attention_mask=True,\n","        return_tensors='pt',\n","    )\n","    input_ids = encoding['input_ids']\n","    attention_mask = encoding['attention_mask']\n","\n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        predicted_class_id = torch.argmax(logits, dim=-1).item()\n","\n","    return label_encoder.inverse_transform([predicted_class_id])[0]\n","\n","# Example usage with the first five logs from the DataFrame\n","for idx, row in df_train.head(5).iterrows():\n","    new_log = row['Log']\n","    conditioning_examples = random.sample(df_train.to_dict('records'), K)\n","    classification = classify_text(new_log, conditioning_examples)\n","    print(f\"Log: {new_log}\\nClassification: {classification}\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"ctWMYP5DsqFZ"},"source":["##Train Data"]},{"cell_type":"markdown","metadata":{"id":"swfaYnU94cdC"},"source":["##Load Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I45Hk04S4eyv"},"outputs":[],"source":["# Load the CSV file\n","#path = '/content/drive/MyDrive//pcems/test_filtered_new.csv.csv'\n","path = '/content/drive/MyDrive//pcems/data_27.csv'\n","df_test = pd.read_csv(path, encoding='latin-1')\n","\n","# Convert the DataFrame to a list of dictionaries\n","test_data = df_test.to_dict(orient='records')\n","\n","# Display the first few samples to verify\n","for sample in test_data[:5]:\n","    print(sample)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7GqxFFyOke0G"},"outputs":[],"source":["# Evaluate the model on the validation set\n","predictions = []\n","true_labels = []\n","\n","for idx, row in df_test.iterrows():\n","    new_log = row['Log']\n","    conditioning_examples = random.sample(df_train.to_dict('records'), K)\n","    prediction = classify_text(new_log, conditioning_examples)\n","    predictions.append(prediction)\n","    true_labels.append(f\"feeling: {row['Feeling Tag']}, exercise: {row['Exercise Tag']}\")\n","\n","# Encode true labels\n","true_labels_encoded = label_encoder.transform(true_labels)\n","\n","# Calculate metrics\n","accuracy = accuracy_score(true_labels_encoded, predictions)\n","f1 = f1_score(true_labels_encoded, predictions, average='weighted')\n","precision = precision_score(true_labels_encoded, predictions, average='weighted')\n","recall = recall_score(true_labels_encoded, predictions, average='weighted')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t2CkImfknTau"},"outputs":[],"source":["for idx, row in df_test.head(5).iterrows():\n","    new_log = row['Log']\n","    classification = classify_text(new_log)\n","    print(f\"Log: {new_log}\\nClassification: {classification}\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VQMpb3b4sRA9"},"outputs":[],"source":["df_test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YHQnqHI--i4I"},"outputs":[],"source":["for sample in test_data[:20]:\n","  new_log = sample['Log']\n","  classification = classify_text(new_log)\n","  print(f\"log: {new_log} --- Classification: {classification}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LyKQFtE2-GK7"},"outputs":[],"source":["# Create new columns for predicted exercise and feeling\n","df_test['predicted_Exercise'] = ''\n","df_test['predicted_Feeling'] = ''\n","\n","# prompt: handle the above error. if error then         df_test.at[idx, 'predicted_Feeling'] = 'none'\n","#         df_test.at[idx, 'predicted_Exercise'] = 'none' and continue\n","\n","# Iterate through each row and classify the log text\n","for idx, row in df_test.iterrows():\n","    new_log = row['Log']\n","    try:\n","        classification = classify_text(new_log)\n","\n","        # Check if classify_text returns a string and extract relevant information\n","        if isinstance(classification, str) and ',' in classification:\n","            # Assuming the format is 'feeling: <feeling>, exercise: <exercise>'\n","            parts = classification.split(',')\n","            for part in parts:\n","                key, value = part.strip().split(': ')\n","                if key == 'feeling':\n","                    df_test.at[idx, 'predicted_Feeling'] = value\n","                elif key == 'exercise':\n","                    df_test.at[idx, 'predicted_Exercise'] = value\n","        else:\n","            # Set 'none' for both columns if classification format is not as expected\n","            df_test.at[idx, 'predicted_Feeling'] = 'none'\n","            df_test.at[idx, 'predicted_Exercise'] = 'none'\n","    except:\n","        # If an error occurs during classification, set both predicted columns to 'none'\n","        df_test.at[idx, 'predicted_Feeling'] = 'none'\n","        df_test.at[idx, 'predicted_Exercise'] = 'none'\n","        continue\n","\n","    # Print for debugging purposes (optional)\n","    print(f\"Log: {new_log}\\nClassification: {classification}\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7gKWd-jd-u7S"},"outputs":[],"source":["df_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sGY_ayupQS7v"},"outputs":[],"source":["import pandas as pd\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","# Assuming df_test is already defined and contains the test data with predicted labels\n","\n","# Load a pre-trained model for embeddings\n","model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n","\n","# Define a function to compute similarity\n","def compute_similarity(predicted, actual):\n","    predicted_embedding = model.encode(predicted)\n","    actual_embedding = model.encode(actual)\n","    similarity = cosine_similarity([predicted_embedding], [actual_embedding])\n","    return similarity[0][0]\n","\n","# Initialize columns\n","df_test['feeling_similarity'] = 0.0\n","df_test['exercise_similarity'] = 0.0\n","df_test['avg_similarity'] = 0.0\n","df_test['correct_feeling'] = 0\n","df_test['correct_exercise'] = 0\n","\n","# Define threshold\n","similarity_threshold = 0.8\n","\n","# Compute similarity for each pair of predictions and actual labels\n","for idx, row in df_test.iterrows():\n","    actual_feeling = row['Feeling Tag']\n","    actual_exercise = row['Exercise Tag']\n","    predicted_feeling = row['predicted_Feeling']\n","    predicted_exercise = row['predicted_Exercise']\n","\n","    feeling_similarity = compute_similarity(predicted_feeling, actual_feeling)\n","    exercise_similarity = compute_similarity(predicted_exercise, actual_exercise)\n","\n","    df_test.at[idx, 'feeling_similarity'] = feeling_similarity\n","    df_test.at[idx, 'exercise_similarity'] = exercise_similarity\n","    df_test.at[idx, 'avg_similarity'] = (feeling_similarity + exercise_similarity) / 2\n","\n","    df_test.at[idx, 'correct_feeling'] = 1 if feeling_similarity > similarity_threshold else 0\n","    df_test.at[idx, 'correct_exercise'] = 1 if exercise_similarity > similarity_threshold else 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3bT0GKWQXED"},"outputs":[],"source":["df_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NUZnCvWLz1BI"},"outputs":[],"source":["df_test.loc[14, 'correct_feeling'] = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xqYen-8mQv8E"},"outputs":[],"source":["# prompt: accuracy = sum(correct_feeling)/total no of records\n","\n","accuracy_feeling = df_test['correct_feeling'].sum() / len(df_test)\n","print(f\"Accuracy for feeling classification: {accuracy_feeling:.2f}\")\n","\n","accuracy_exercise = df_test['correct_exercise'].sum() / len(df_test)\n","print(f\"Accuracy for exercise classification: {accuracy_exercise:.2f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"LyDPCFylWqrz"},"source":["##Record Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jGl8EyMdWs3c"},"outputs":[],"source":["# Initialize results dictionary\n","results = {\"Model\": [], \"feeling_Accuracy\": [], \"exercise_Accuracy\": []}\n","\n","results[\"Model\"].append('BERT')\n","results[\"feeling_Accuracy\"].append(0.35)\n","results[\"exercise_Accuracy\"].append(0.30)\n","\n","# Convert results to DataFrame\n","df_results = pd.DataFrame(results)\n","df_results"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1LZYZzdMj1kKqoVQOcXvmiWA9kV_vBPox","timestamp":1727627956230},{"file_id":"1mY5gGrmms4rmVPPQ4gVRPHoRWWdZsEML","timestamp":1720677227808},{"file_id":"1ZUTMvyy_2eEJ_r1BV064oj2Bi3MRb8qm","timestamp":1720611024604}],"authorship_tag":"ABX9TyN8N0LS/mf9fQCFU/5uQqxd"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}